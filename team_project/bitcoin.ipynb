{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (22.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# poloniex's API에 연결합니다.\n",
    "url = 'https://poloniex.com/public?command=returnChartData&currencyPair=USDT_BTC&start=1356998100&end=9999999999&period=300'\n",
    "\n",
    "# API를 통해 얻은 json을 파싱하고, pandas의 DataFrame으로 바꿔줍니다.\n",
    "openUrl = urlopen(url)\n",
    "r = openUrl.read()\n",
    "openUrl.close()\n",
    "d = json.loads(r.decode())\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "original_columns=[u'close', u'date', u'high', u'low', u'open']\n",
    "new_columns = ['Close','Timestamp','High','Low','Open']\n",
    "df = df.loc[:,original_columns]\n",
    "df.columns = new_columns\n",
    "df.to_csv('bitcoin2015to2017.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PastSampler:\n",
    "    '''\n",
    "    학습 데이터(training samples)를 과거 데이터를 이용해서 미래를 예측할 수 있도록 형태를 갖춰줍니다.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, N, K, sliding_window = True):\n",
    "        '''\n",
    "        N개의 과거 데이터를 이용해 K개의 미래를 예측합니다.\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.sliding_window = sliding_window\n",
    "\n",
    "    def transform(self, A):\n",
    "        M = self.N + self.K     #한개의 row당 데이터 개수 (sample + target)\n",
    "        #indexes\n",
    "        if self.sliding_window:\n",
    "            I = np.arange(M) + np.arange(A.shape[0] - M + 1).reshape(-1, 1)\n",
    "        else:\n",
    "            if A.shape[0]%M == 0:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)\n",
    "\n",
    "            else:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)\n",
    "\n",
    "        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])\n",
    "        ci = self.N * A.shape[1]    #한 데이터당 feature 개수\n",
    "        return B[:, :ci], B[:, ci:] #학습 matrix, 타겟 matrix\n",
    "\n",
    "#데이터 파일 위치(path)\n",
    "dfp = 'bitcoin2015to2017.csv'\n",
    "\n",
    "# 가격 데이터 컬럼(열, columns)\n",
    "columns = ['Close']\n",
    "df = pd.read_csv(dfp)\n",
    "time_stamps = df['Timestamp']\n",
    "df = df.loc[:,columns]\n",
    "original_df = pd.read_csv(dfp).loc[:,columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2366 sha256=4ddc22cbb57808929784aff8054313e2ff20c579a4715308a3736ee08a4739b0\n",
      "  Stored in directory: c:\\users\\myfamily\\appdata\\local\\pip\\cache\\wheels\\db\\9f\\0b\\772886b624f84c138a5febb6966c89d374ab58c62bd65d109e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: h5py in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\myfamily\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h5py) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sklearn\n",
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbitcoin2015to2017_close.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# normalization\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "file_name='bitcoin2015to2017_close.h5'\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# normalization\n",
    "for c in columns:\n",
    "    df[c] = scaler.fit_transform(df[c].values.reshape(-1,1))\n",
    "\n",
    "#Features are input sample dimensions(channels)\n",
    "A = np.array(df)[:,None,:]\n",
    "original_A = np.array(original_df)[:,None,:]\n",
    "time_stamps = np.array(time_stamps)[:,None,None]\n",
    "\n",
    "#Make samples of temporal sequences of pricing data (channel)\n",
    "NPS, NFS = 256, 16         #과거 데이터, 미래 데이터 개수\n",
    "ps = PastSampler(NPS, NFS, sliding_window=False)\n",
    "B, Y = ps.transform(A)\n",
    "input_times, output_times = ps.transform(time_stamps)\n",
    "original_B, original_Y = ps.transform(original_A)\n",
    "\n",
    "import h5py\n",
    "with h5py.File(file_name, 'w') as f:\n",
    "    f.create_dataset(\"inputs\", data = B)\n",
    "    f.create_dataset('outputs', data = Y)\n",
    "    f.create_dataset(\"input_times\", data = input_times)\n",
    "    f.create_dataset('output_times', data = output_times)\n",
    "    f.create_dataset(\"original_datas\", data=np.array(original_df))\n",
    "    f.create_dataset('original_inputs',data=original_B)\n",
    "    f.create_dataset('original_outputs',data=original_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m set_session(tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mSession(config\u001b[39m=\u001b[39mconfig))\n\u001b[0;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39m'\u001b[39m\u001b[39mbitcoin2015to2017_close.h5\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m hf:\n\u001b[1;32m---> 24\u001b[0m     datas \u001b[39m=\u001b[39m hf[\u001b[39m'\u001b[39;49m\u001b[39minputs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalue\n\u001b[0;32m     25\u001b[0m     labels \u001b[39m=\u001b[39m hf[\u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue\n\u001b[0;32m     27\u001b[0m output_file_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbitcoin2015to2017_close_CNN_2_relu\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, LeakyReLU, PReLU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.backend import set_session\n",
    "\n",
    "# 한 개의 GPU만을 사용하도록 설정\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] ='2'\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "with h5py.File(''.join(['bitcoin2015to2017_close.h5']), 'r') as hf:\n",
    "    datas = hf['inputs'].value\n",
    "    labels = hf['outputs'].value\n",
    "\n",
    "output_file_name='bitcoin2015to2017_close_CNN_2_relu'\n",
    "\n",
    "step_size = datas.shape[1]\n",
    "batch_size= 8\n",
    "nb_features = datas.shape[2]\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# 데이터를 train, validation 으로 나눔\n",
    "training_size = int(0.8* datas.shape[0])\n",
    "training_datas = datas[:training_size,:]\n",
    "training_labels = labels[:training_size,:]\n",
    "validation_datas = datas[training_size:,:]\n",
    "validation_labels = labels[training_size:,:]\n",
    "#build model\n",
    "\n",
    "# 2 layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(activation='relu', input_shape=(step_size, nb_features), strides=3, filters=8, kernel_size=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D( strides=4, filters=nb_features, kernel_size=16))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(training_datas, training_labels,verbose=1, batch_size=batch_size,validation_data=(validation_datas,validation_labels), epochs = epochs, callbacks=[CSVLogger(output_file_name+'.csv', append=True),ModelCheckpoint('weights/'+output_file_name+'-{epoch:02d}-{val_loss:.5f}.hdf5', monitor='val_loss', verbose=1,mode='min')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
